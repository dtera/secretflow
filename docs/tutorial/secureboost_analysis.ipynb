{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertically Federated XGB (SecureBoost) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The following codes are demos only. It's **NOT for production** due to system security concerns, please **DO NOT** use it directly in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with XGB\n",
    "\n",
    "In this notebook, we are going to compare the performance of our implementation of secureboost vs XGB in cleartext.\n",
    "\n",
    "In the end we will give a comparison between the two models on the same datasets with respect to AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import pprint\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import secretflow as sf\n",
    "import spu\n",
    "import xgboost as xgb\n",
    "from secretflow.data import FedNdarray, partition, PartitionWay\n",
    "from secretflow.data.split import train_test_split as train_test_split_fed\n",
    "from secretflow.data.vertical import VDataFrame\n",
    "from secretflow.device.driver import reveal, wait\n",
    "from secretflow.ml.boost.sgb_v import (\n",
    "    get_classic_lightGBM_params,\n",
    "    get_classic_XGB_params,\n",
    "    Sgb,\n",
    ")\n",
    "\n",
    "from secretflow.ml.boost.sgb_v.core.params import xgb_params_converter\n",
    "from secretflow.ml.boost.sgb_v.model import load_model\n",
    "from secretflow.preprocessing import LabelEncoder\n",
    "from secretflow.utils.simulation.datasets import (\n",
    "    load_bank_marketing,\n",
    "    load_bank_marketing_unpartitioned,\n",
    "    load_creditcard,\n",
    "    load_creditcard_unpartitioned,\n",
    ")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder as SkLabelEncoder\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "\n",
    "# Check the version of your SecretFlow\n",
    "print('The version of SecretFlow: {}'.format(sf.__version__))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "alice_ip = '127.0.0.1'\n",
    "bob_ip = '127.0.0.1'\n",
    "ip_party_map = {bob_ip: 'bob', alice_ip: 'alice'}\n",
    "\n",
    "_system_config = {'lineage_pinning_enabled': False}\n",
    "sf.shutdown()\n",
    "# init cluster\n",
    "sf.init(\n",
    "    ['alice', 'bob'],\n",
    "    address='local',\n",
    "    _system_config=_system_config,\n",
    "    object_store_memory=50 * 1024 * 1024 * 1024,\n",
    ")\n",
    "\n",
    "# SPU settings\n",
    "cluster_def = {\n",
    "    'nodes': [\n",
    "        {'party': 'alice', 'id': 'local:0', 'address': alice_ip + ':12945'},\n",
    "        {'party': 'bob', 'id': 'local:1', 'address': bob_ip + ':12946'},\n",
    "        # {'party': 'carol', 'id': 'local:2', 'address': '127.0.0.1:12347'},\n",
    "    ],\n",
    "    'runtime_config': {\n",
    "        # SEMI2K support 2/3 PC, ABY3 only support 3PC, CHEETAH only support 2PC.\n",
    "        # pls pay attention to size of nodes above. nodes size need match to PC setting.\n",
    "        'protocol': spu.ProtocolKind.SEMI2K,\n",
    "        'field': spu.FieldType.FM128,\n",
    "    },\n",
    "}\n",
    "\n",
    "# HEU settings\n",
    "heu_config = {\n",
    "    'sk_keeper': {'party': 'alice'},\n",
    "    'evaluators': [{'party': 'bob'}],\n",
    "    'mode': 'PHEU',\n",
    "    'he_parameters': {\n",
    "        # ou is a fast encryption schema that is as secure as paillier.\n",
    "        'schema': 'ou',\n",
    "        'key_pair': {\n",
    "            'generate': {\n",
    "                # bit size should be 2048 to provide sufficient security.\n",
    "                'bit_size': 2048,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'encoding': {\n",
    "        'cleartext_type': 'DT_I32',\n",
    "        'encoder': \"IntegerEncoder\",\n",
    "        'encoder_args': {\"scale\": 1},\n",
    "    },\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "alice = sf.PYU('alice')\n",
    "bob = sf.PYU('bob')\n",
    "heu = sf.HEU(heu_config, cluster_def['runtime_config']['field'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "data = load_bank_marketing(parts={alice: (0, 16), bob: (16, 16)}, axis=1, full=True)\n",
    "label = load_bank_marketing(parts={alice: (16, 17)}, axis=1, full=True)\n",
    "\n",
    "bank_unpartitioned = load_bank_marketing_unpartitioned(full=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "bank_unpartitioned.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "bank_unpartitioned.head"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "bank_unpartitioned['y']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# preprocess the data\n",
    "encoder = LabelEncoder()\n",
    "data['job'] = encoder.fit_transform(data['job'])\n",
    "data['marital'] = encoder.fit_transform(data['marital'])\n",
    "data['education'] = encoder.fit_transform(data['education'])\n",
    "data['default'] = encoder.fit_transform(data['default'])\n",
    "data['housing'] = encoder.fit_transform(data['housing'])\n",
    "data['loan'] = encoder.fit_transform(data['loan'])\n",
    "data['contact'] = encoder.fit_transform(data['contact'])\n",
    "data['poutcome'] = encoder.fit_transform(data['poutcome'])\n",
    "data['month'] = encoder.fit_transform(data['month'])\n",
    "label = encoder.fit_transform(label)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "encoder = SkLabelEncoder()\n",
    "for col in [\n",
    "    'job',\n",
    "    'marital',\n",
    "    'education',\n",
    "    'default',\n",
    "    'housing',\n",
    "    'loan',\n",
    "    'contact',\n",
    "    'month',\n",
    "    'poutcome',\n",
    "    'y',\n",
    "]:\n",
    "    bank_unpartitioned[col] = encoder.fit_transform(bank_unpartitioned[col])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "bank_unpartitioned.head"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best convergence round with XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "X = bank_unpartitioned.iloc[:, :-1]\n",
    "y = bank_unpartitioned.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=94\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=94\n",
    ")\n",
    "\n",
    "# Use \"hist\" for constructing the trees, with early stopping enabled.\n",
    "clf = xgb.XGBClassifier(\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.3,\n",
    "    max_bin=10,\n",
    "    early_stopping_rounds=5,\n",
    "    base_score=0.5,\n",
    "    eval_metric=\"auc\",\n",
    "    reg_lambda=0.1,\n",
    "    min_child_weight=0,\n",
    ")\n",
    "clf.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "\n",
    "print(\n",
    "    \"train set AUC score: \",\n",
    "    roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1]),\n",
    "    \"test set AUC score: \",\n",
    "    roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]),\n",
    "    \"num_trees: \",\n",
    "    clf.best_iteration + 1,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# train on the whole dataset and measure performance\n",
    "\n",
    "# Use \"hist\" for constructing the trees, with early stopping enabled.\n",
    "clf2 = xgb.XGBClassifier(\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=clf.best_iteration + 1,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.3,\n",
    "    max_bin=10,\n",
    "    eval_metric=\"auc\",\n",
    "    base_score=0.5,\n",
    "    reg_lambda=0.1,\n",
    "    min_child_weight=0,\n",
    ")\n",
    "clf2.fit(X, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "print(\n",
    "    \"train set AUC score: \",\n",
    "    roc_auc_score(y, clf2.predict_proba(X)[:, 1]),\n",
    "    \"num_trees: \",\n",
    "    clf2.best_iteration + 1,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "X_train_fed, X_test_fed = train_test_split_fed(data, test_size=0.2, random_state=94)\n",
    "y_train_fed, y_test_fed = train_test_split_fed(label, test_size=0.2, random_state=94)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to do the same using sgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "sgb = Sgb(heu)\n",
    "params = get_classic_XGB_params()\n",
    "params['num_boost_round'] = 100\n",
    "params['max_depth'] = 5\n",
    "params['base_score'] = 0.5\n",
    "params['reg_lambda'] = 0.1\n",
    "params['learning_rate'] = 0.3\n",
    "params['sketch_eps'] = 1 / 10\n",
    "params['enable_early_stop'] = True\n",
    "params['enable_monitor'] = True\n",
    "params['validation_fraction'] = 0.2\n",
    "params['stopping_rounds'] = 5\n",
    "params['stopping_tolerance'] = 0.0000000001\n",
    "params['seed'] = 94\n",
    "params['first_tree_with_label_holder_feature'] = False\n",
    "params['save_best_model'] = True\n",
    "\n",
    "model = sgb.train(params, X_train_fed, y_train_fed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "print(\n",
    "    \"train set AUC score: \",\n",
    "    roc_auc_score(\n",
    "        reveal(y_train_fed.partitions[alice].data), reveal(model.predict(X_train_fed))\n",
    "    ),\n",
    "    \"test set AUC score: \",\n",
    "    roc_auc_score(\n",
    "        reveal(y_test_fed.partitions[alice].data), reveal(model.predict(X_test_fed))\n",
    "    ),\n",
    "    \"num_trees: \",\n",
    "    len(model.get_trees()),\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "sgb = Sgb(heu)\n",
    "params = get_classic_XGB_params()\n",
    "params['num_boost_round'] = clf.best_iteration + 1\n",
    "params['max_depth'] = 5\n",
    "params['base_score'] = 0.5\n",
    "params['learning_rate'] = 0.3\n",
    "params['sketch_eps'] = 1 / 10\n",
    "params['seed'] = 94\n",
    "params['first_tree_with_label_holder_feature'] = False\n",
    "\n",
    "model2 = sgb.train(params, data, label)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "print(\n",
    "    \"train set AUC score: \",\n",
    "    roc_auc_score(reveal(label.partitions[alice].data), reveal(model2.predict(data))),\n",
    "    \"num_trees: \",\n",
    "    len(model2.get_trees()),\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline the test run\n",
    "\n",
    "We have performed one test run on the bank dataset. In this test, we use the same parameters for XGB and SGB and compare the model results.\n",
    "There are several steps:\n",
    "\n",
    "1. run XGB on a set of params on training data with early stopping enabled\n",
    "2. run SGB on a set of params on training data with early stopping enabled\n",
    "3. run XGB on the whole training data (no early stopping) with the optimal rounds from step 1\n",
    "4. run SGB on the whole training data (no early stopping) with the optimal rounds from step 1\n",
    "6. collect results of convergence rounds and AUC scores\n",
    "\n",
    "We have found the convergence rounds are close and AUC scores are similar between XGB and SGB, therefore add the evidence SGB is similar to XGB in terms of accuracy.\n",
    "\n",
    "However, a single data point is not enough to make a conclusion about the performance of SGB.\n",
    "It is possible to perform multiple runs and collect the data.\n",
    "\n",
    "Now we are going to pipeline the test run process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "# we begin with a single np dataset\n",
    "# we use label encoding for all the categorical variables\n",
    "# we separate the labels from the features\n",
    "# we then give it to alice and bob by halves, in particular give alice the label\n",
    "\n",
    "\n",
    "def preprocess_data(dataset: pd.DataFrame, label_column: str):\n",
    "    # perform label encoding on categorical variables\n",
    "    for col in dataset.columns:\n",
    "        if isinstance(dataset[col][0], str):\n",
    "            le = SkLabelEncoder()\n",
    "            dataset[col] = le.fit_transform(dataset[col])\n",
    "\n",
    "    # separate labels from features\n",
    "    X = dataset.drop(label_column, axis=1)\n",
    "    y = dataset[label_column]\n",
    "\n",
    "    X_col_num = X.shape[1]\n",
    "    split_count = int(X_col_num / 2)\n",
    "    vdata = VDataFrame(\n",
    "        partitions={\n",
    "            alice: partition(alice(lambda x: x)(X.iloc[:, :split_count])),\n",
    "            bob: partition(bob(lambda x: x)(X.iloc[:, split_count:])),\n",
    "        }\n",
    "    )\n",
    "    label = VDataFrame(partitions={alice: partition(alice(lambda x: x)(y))})\n",
    "\n",
    "    return X, y, vdata, label\n",
    "\n",
    "\n",
    "DEFAULT_XGB_PARAMS = {\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.3,\n",
    "    \"max_bin\": 10,\n",
    "    \"early_stopping_rounds\": 5,\n",
    "    \"base_score\": 0.5,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"min_child_weight\": 0,\n",
    "    \"random_state\": 95,\n",
    "}\n",
    "\n",
    "\n",
    "def fit_xgb(X, y, params, valid_frac=0.2, test_frac=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_frac, random_state=params[\"random_state\"]\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=valid_frac, random_state=params[\"random_state\"]\n",
    "    )\n",
    "\n",
    "    # Use \"hist\" for constructing the trees, with early stopping enabled.\n",
    "    clf = xgb.XGBClassifier(**params)\n",
    "    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    return clf, X_test, y_test\n",
    "\n",
    "\n",
    "def find_xgb_auc_and_best_iter_round(clf, X_test, y_test):\n",
    "    converge_num_trees = clf.best_iteration + 1\n",
    "    converge_test_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    return converge_test_auc, converge_num_trees\n",
    "\n",
    "\n",
    "def fit_sgb(vdata, label, params, valid_frac=0.2, test_frac=0.2):\n",
    "    X_train_fed, X_test_fed = train_test_split_fed(\n",
    "        vdata, test_size=test_frac, random_state=params[\"random_state\"]\n",
    "    )\n",
    "    y_train_fed, y_test_fed = train_test_split_fed(\n",
    "        label, test_size=test_frac, random_state=params[\"random_state\"]\n",
    "    )\n",
    "    sgb = Sgb(heu)\n",
    "    sgb_params = xgb_params_converter(params)\n",
    "    sgb_params['validation_fraction'] = valid_frac\n",
    "    model = sgb.train(sgb_params, X_train_fed, y_train_fed)\n",
    "    return model, X_test_fed, y_test_fed\n",
    "\n",
    "\n",
    "def find_sgb_auc_and_best_iter_round(model, X_test_fed, y_test_fed):\n",
    "    test_auc = roc_auc_score(\n",
    "        reveal(y_test_fed.partitions[alice].data), reveal(model.predict(X_test_fed))\n",
    "    )\n",
    "    num_trees = len(model.get_trees())\n",
    "    return test_auc, num_trees\n",
    "\n",
    "\n",
    "def find_sgb_auc_at_xgb_convergence_point(\n",
    "    model, X_test_fed, y_test_fed, xgb_converge_num_trees\n",
    "):\n",
    "    return roc_auc_score(\n",
    "        reveal(y_test_fed.partitions[alice].data),\n",
    "        reveal(model[:xgb_converge_num_trees].predict(X_test_fed)),\n",
    "    )\n",
    "\n",
    "\n",
    "class ExperimentResult:\n",
    "    def __init__(self):\n",
    "        self.xgb_test_auc = 0\n",
    "        self.xgb_num_trees = 0\n",
    "        self.xgboost_fit_time = 0\n",
    "\n",
    "        self.sgb_test_auc = 0\n",
    "        self.sgb_num_trees = 0\n",
    "        self.sgb_fit_time = 0\n",
    "        self.sgb_test_auc_at_xgb_convergence = 0\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'xgb_test_auc': self.xgb_test_auc,\n",
    "            'xgb_num_trees': self.xgb_num_trees,\n",
    "            'xgboost_fit_time': self.xgboost_fit_time,\n",
    "            'sgb_test_auc': self.sgb_test_auc,\n",
    "            'sgb_num_trees': self.sgb_num_trees,\n",
    "            'sgb_fit_time': self.sgb_fit_time,\n",
    "            'sgb_test_auc_at_xgb_convergence': self.sgb_test_auc_at_xgb_convergence,\n",
    "        }\n",
    "\n",
    "\n",
    "def run_experiement(\n",
    "    dataset: pd.DataFrame,\n",
    "    label_column: str,\n",
    "    params: dict,\n",
    "    experiment_name: str,\n",
    "    valid_frac=0.2,\n",
    "    test_frac=0.2,\n",
    "):\n",
    "    X, y, vdata, label = preprocess_data(dataset, label_column)\n",
    "    print(\"Starting {}\".format(experiment_name))\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    clf, X_test, y_test = fit_xgb(X, y, params, valid_frac, test_frac)\n",
    "    xgboost_fit_time = time.perf_counter() - start\n",
    "    xgb_test_auc, xgb_num_trees = find_xgb_auc_and_best_iter_round(clf, X_test, y_test)\n",
    "    print(\"XGBoost Test AUC: {}, Num Trees: {}\".format(xgb_test_auc, xgb_num_trees))\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    model, X_test_fed, y_test_fed = fit_sgb(vdata, label, params, valid_frac, test_frac)\n",
    "    sgb_fit_time = time.perf_counter() - start\n",
    "    sgb_test_auc, sgb_num_trees = find_sgb_auc_and_best_iter_round(\n",
    "        model, X_test_fed, y_test_fed\n",
    "    )\n",
    "    print(\"SGB Test AUC: {}, Num Trees: {}\".format(sgb_test_auc, sgb_num_trees))\n",
    "\n",
    "    sgb_test_auc_at_xgb_convergence = find_sgb_auc_at_xgb_convergence_point(\n",
    "        model, X_test_fed, y_test_fed, xgb_num_trees\n",
    "    )\n",
    "    print(\n",
    "        \"SGB Test AUC at XGB Convergence Point: {}\".format(\n",
    "            sgb_test_auc_at_xgb_convergence\n",
    "        )\n",
    "    )\n",
    "\n",
    "    experiment_result = ExperimentResult()\n",
    "\n",
    "    experiment_result.xgb_test_auc = xgb_test_auc\n",
    "    experiment_result.xgb_num_trees = xgb_num_trees\n",
    "    experiment_result.xgboost_fit_time = xgboost_fit_time\n",
    "\n",
    "    experiment_result.sgb_test_auc = sgb_test_auc\n",
    "    experiment_result.sgb_num_trees = sgb_num_trees\n",
    "    experiment_result.sgb_fit_time = sgb_fit_time\n",
    "\n",
    "    experiment_result.sgb_test_auc_at_xgb_convergence = sgb_test_auc_at_xgb_convergence\n",
    "    return experiment_result\n",
    "\n",
    "\n",
    "def collect_results(results: List[ExperimentResult]):\n",
    "    results_dict = [r.to_dict() for r in results]\n",
    "    return pd.DataFrame(results_dict)\n",
    "\n",
    "\n",
    "def run_repeated_experiments(\n",
    "    dataset: pd.DataFrame,\n",
    "    label_column: str,\n",
    "    params: dict,\n",
    "    experiment_name: str,\n",
    "    num_repeats: int,\n",
    "    valid_frac=0.2,\n",
    "    test_frac=0.2,\n",
    "    seed=1212,\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    results = []\n",
    "    for i in range(num_repeats):\n",
    "        random_state = np.random.randint(low=0, high=1000000)\n",
    "        params[\"random_state\"] = random_state\n",
    "        result = run_experiement(\n",
    "            dataset,\n",
    "            label_column,\n",
    "            params,\n",
    "            experiment_name + \"_repeat_\" + str(i + 1),\n",
    "            valid_frac,\n",
    "            test_frac,\n",
    "        )\n",
    "        results.append(result)\n",
    "    return collect_results(results)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# outputs are cleared in web demo for viewing purposes\n",
    "\n",
    "creditcard_experiement_results_table = run_repeated_experiments(\n",
    "    load_creditcard_unpartitioned(), 'Class', DEFAULT_XGB_PARAMS, 'creditcard', 20\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# outputs are cleared in web demo for viewing purposes\n",
    "\n",
    "bank_marketing_experiement_results_table = run_repeated_experiments(\n",
    "    load_bank_marketing_unpartitioned(full=True),\n",
    "    'y',\n",
    "    DEFAULT_XGB_PARAMS,\n",
    "    'bank_marketing',\n",
    "    15,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "creditcard_experiement_results_table.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "bank_marketing_experiement_results_table.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def xy_to_dataframe(X, y, feature_names=None):\n",
    "    \"\"\"\n",
    "    Convert (X, y) into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - X (array-like): A two-dimensional array-like structure containing feature data.\n",
    "    - y (array-like): A one-dimensional array-like structure containing target variable.\n",
    "    - feature_names (list of str, optional): A list of feature names for the DataFrame columns. If None, generic names are used.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): A pandas DataFrame containing X and y combined.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if feature_names is provided; if not, create generic feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(1, len(X[0]) + 1)]\n",
    "    elif len(feature_names) != len(X[0]):\n",
    "        raise ValueError(\n",
    "            \"Length of feature_names does not match number of features in X.\"\n",
    "        )\n",
    "\n",
    "    # Convert X and y into a pandas DataFrame\n",
    "    df_X = pd.DataFrame(X, columns=feature_names)\n",
    "    df_y = pd.Series(y, name='target')\n",
    "\n",
    "    # Concatenate X and y dataframes\n",
    "    df = pd.concat([df_X, df_y], axis=1)\n",
    "\n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# outputs are cleared for viewing purpose\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "random_200w_200d_experiement_results_table = run_repeated_experiments(\n",
    "    xy_to_dataframe(\n",
    "        *make_classification(n_samples=200 * 10000, n_features=200, random_state=42)\n",
    "    ),\n",
    "    'target',\n",
    "    DEFAULT_XGB_PARAMS,\n",
    "    'random_200w_200d',\n",
    "    10,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "random_200w_200d_experiement_results_table.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As we can see, the sgb can perform similarly to XGBoost models. \n",
    "\n",
    "However, it is not as fast as XGBoost. The ratio between the time consumptions can range from 8 to 12 times in a LAN setting.\n",
    "\n",
    "Welcome to contribute and run more analysis on more datasets and parameters!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
